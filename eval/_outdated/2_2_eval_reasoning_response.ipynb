{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1cnUzXrtwR80GIbPJZCv6yBFkR0OdSl6J","authorship_tag":"ABX9TyNIp31WIq8yQwOwB6qRzk4c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VgoCyjrA1MRC"},"outputs":[],"source":["from openai import OpenAI\n","import pandas as pd\n","import json\n","import os\n","\n","client = OpenAI(\n","  api_key=\"REDACTED\"\n",")\n"]},{"cell_type":"code","source":["# 질문에 대한 모델의 답변 생성 함수\n","def generate_answer(model, expand_context, question, gold_answer, answer):\n","\n","    prompt_improved = f\"\"\"\n","      LLM 응답 품질 평가 전문가입니다. 답변을 전문가 검수 정답과 비교하여 평가합니다.\n","\n","      **평가 대상:**\n","      - 질문: {question}\n","      - 평가할 답변: {answer}\n","      - 정답 (전문가 검수): {gold_answer}\n","\n","      **평가 방식:**\n","      1. 정답이 사용한 문서 정보를 파악\n","      2. 답변이 동일한 정보와 논리를 사용했는지 비교\n","      3. 결론이 정답과 다르면 감점\n","\n","      **중요: 아래 참조 문서에서 정답이 어떤 정보를 사용했는지 반드시 확인하세요.**\n","\n","      **참조 문서:**\n","      {expand_context}\n","\n","      ---\n","\n","      위 문서를 기반으로 다음 평가 기준에 따라 점수를 매기세요.\n","\n","      **평가 기준 (각 1-10점):**\n","\n","      1. **정합성**: 정답이 사용한 관련 정보를 답변도 사용했는가?\n","        - 1-3: 정답과 다른 무관한 정보 다수 사용, 핵심 문맥 왜곡\n","        - 4-6: 일부 정답과 같은 정보 사용하나 무관한 내용도 포함\n","        - 7-8: 대부분 정답과 동일한 정보 사용, 소수 부차적 정보 혼입\n","        - 9-10: 정답과 동일한 정보만 선별적 활용, 불필요한 문맥 배제\n","\n","      2. **일관성**: 정답처럼 주제 초점이 흔들리지 않는가?\n","        - 1-3: 주제가 빈번히 전환됨, 정답과 다른 방향\n","        - 4-6: 전반적 일관성 있으나 비관련 내용 간헐적 혼입\n","        - 7-8: 정답과 유사하게 톤과 논점 유지, 약간의 주변 정보\n","        - 9-10: 정답처럼 처음부터 끝까지 주제 집중, 논리적 일관성\n","\n","      3. **정확성**: 정답의 사실과 일치하는가?\n","        - 1-3: 정답과 다른 사실 제시, 무관한 정보를 사실로 제시\n","        - 4-6: 주요 사실은 정답과 맞으나 일부 세부 오류\n","        - 7-8: 정답과 전반적으로 일치, 약간의 모호함\n","        - 9-10: 정답의 사실과 완전히 일치\n","\n","      4. **완전성**: 정답이 다룬 핵심 쟁점을 빠짐없이 다루었는가?\n","        - 1-3: 정답의 핵심 요소 대부분 누락\n","        - 4-6: 정답의 주요 쟁점은 언급했으나 하위 맥락 부족\n","        - 7-8: 정답의 대부분 측면을 다루나 부차적 요소 약함\n","        - 9-10: 정답처럼 모든 측면을 빠짐없이 포괄적으로 다룸\n","\n","      5. **추론성**: 정답과 유사한 논리적 추론 과정을 보이는가?\n","        - 1-3: 정답과 다른 추론, 과정 없거나 비논리적\n","        - 4-6: 일부 정답과 유사하나 논리 비약 발생\n","        - 7-8: 정답과 대부분 유사한 자연스러운 추론\n","        - 9-10: 정답과 동일한 단계별 명확한 논리 흐름\n","\n","      6. **전체 품질**: 정답과의 전반적 일치도\n","        - 1-3: 정답과 큰 차이, 전반적 품질 낮음\n","        - 4-6: 일정 수준 일치하나 논리 흐름 불완전\n","        - 7-8: 정답과 유사하게 잘 구성, 소수 불일치\n","        - 9-10: 정답과 높은 일치도, 완성도 높음\n","\n","      **출력 형식 (JSON):**\n","      {{\n","        \"coherence\": 점수,\n","        \"consistency\": 점수,\n","        \"accuracy\": 점수,\n","        \"completeness\": 점수,\n","        \"reasoning\": 점수,\n","        \"overall_quality\": 점수,\n","        \"explanation\": \"정답과의 주요 차이점과 일치 정도 요약 (2-3문장)\"\n","      }}\n","      \"\"\"\n","\n","    completion = client.chat.completions.create(\n","      model=model,\n","      store=True,\n","      messages=[{\"role\": \"system\", \"content\": \"You are a financial domain expert. Your task is to evaluate the clarity and difficulty \"\n","                      \"of the given LLM response based on your expertise in finance.\"},\n","                    {\"role\": \"user\", \"content\": prompt_improved}],\n","    )\n","\n","    return completion.choices[0].message.content"],"metadata":{"id":"8L5tHYi91OS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# CSV 파일 처리\n","def process_csv(input_csv_path, output_csv_path):\n","    # 입력 CSV 파일 읽기\n","    data = pd.read_csv(input_csv_path)\n","\n","    # 결과 저장용 리스트\n","    results = []\n","    num = 1\n","    error_ids = []\n","\n","    headers = data.columns.tolist()\n","    for index, row in tqdm(data.iterrows()):\n","        try:\n","          _id = row['id']\n","\n","          question = row['question']\n","          gold_answer = ['gold_answer']\n","\n","          shuffled_balance_text = row['shuffled_balance_text']\n","          shuffled_cluster_front_text = row['shuffled_cluster_front_text']\n","          shuffled_cluster_middle_text = row['shuffled_cluster_middle_text']\n","          shuffled_cluster_end_text = row['shuffled_cluster_end_text']\n","          shuffled_random_text = row['shuffled_random_text']\n","\n","          answer_balance = row['gpt-oss:120b_answer_balance']\n","          answer_front = row['gpt-oss:120b_answer_front']\n","          answer_middle = row['gpt-oss:120b_answer_middle']\n","          answer_end = row['gpt-oss:120b_answer_end']\n","          answer_random = row['gpt-oss:120b_answer_random']\n","\n","          # gpt-5 모델 평가답변 생성\n","          answer_balance_eval = generate_answer('gpt-5', shuffled_balance_text, question, gold_answer, answer_balance)\n","\n","          answer_front_eval = generate_answer('gpt-5', shuffled_cluster_front_text, question, gold_answer, answer_front)\n","\n","          answer_middle_eval = generate_answer('gpt-5', shuffled_cluster_middle_text, question, gold_answer, answer_middle)\n","\n","          answer_end_eval = generate_answer('gpt-5', shuffled_cluster_end_text, question, gold_answer, answer_end)\n","\n","          answer_random_eval = generate_answer('gpt-5', shuffled_random_text, question, gold_answer, answer_random)\n","\n","\n","\n","          # 결과 저장 -> 각 모델별로\n","          results.append({\n","              \"id\": _id,\n","              \"answer_balance_eval\": answer_balance_eval,\n","              \"answer_front_eval\": answer_front_eval,\n","              \"answer_middle_eval\": answer_middle_eval,\n","              \"answer_end_eval\": answer_end_eval,\n","              \"answer_random_eval\": answer_random_eval,\n","          })\n","\n","          num += 1\n","\n","        except Exception as e:\n","          print(\"error : \", e)\n","          error_ids.append(index)\n","          num += 1\n","          continue\n","\n","    print(f\"{num}개 데이터 평가 완료\")\n","\n","    # 결과를 DataFrame으로 변환 후 CSV 파일로 저장\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(output_csv_path, index=False, encoding='utf-8')"],"metadata":{"id":"Qn3o5A7m1QV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 실행 예시\n","if __name__ == \"__main__\":\n","    # 예제 데이터 경로\n","    file_path = \"./drive/MyDrive/Colab Notebooks/독자AI/추론/12번_fn\"\n","    file_list = os.listdir(file_path)\n","\n","\n","    for file_name in file_list:\n","      print(file_name)\n","      if \"gpt-oss\" not in file_name:\n","        continue\n","      print(f\"{file_name} Start\")\n","      input_csv_path = f\"{file_path}/{file_name}\"   # 입력 CSV 파일 경로\n","      output_csv_path = f\"./drive/MyDrive/Colab Notebooks/독자AI/추론/12번_fn_답변평가/{file_name}_gpt-5_eval_v2.csv\"  # 출력 CSV 파일 경로 (결과 저장용)\n","\n","      # CSV 처리 실행\n","      process_csv(input_csv_path, output_csv_path)\n","\n","      print(f\"{file_name} Done\")\n","      print()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLVDsoyz1SKz","outputId":"800b1d77-3620-4126-985d-0467c03878d1","executionInfo":{"status":"ok","timestamp":1764143520492,"user_tz":-540,"elapsed":11054368,"user":{"displayName":"황보광","userId":"01934291390028843753"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(FN)12_70건_전처리 및 셔플 완료.csv_qwen3_30b-a3b-instruct-2507-fp16-reasoning.csv\n","(FN)12_70건_전처리 및 셔플 완료.csv_deepseek-r1_70b_reasoning.csv\n","(FN)12_70건_전처리 및 셔플 완료.csv_gpt-oss-120b-reasoning.csv\n","(FN)12_70건_전처리 및 셔플 완료.csv_gpt-oss-120b-reasoning.csv Start\n"]},{"output_type":"stream","name":"stderr","text":["70it [3:04:12, 157.89s/it]"]},{"output_type":"stream","name":"stdout","text":["71개 데이터 평가 완료\n","(FN)12_70건_전처리 및 셔플 완료.csv_gpt-oss-120b-reasoning.csv Done\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GLcChgnD2AiH"},"execution_count":null,"outputs":[]}]}